<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="周的小站" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2">






<meta name="description" content="周的小站">
<meta property="og:type" content="website">
<meta property="og:title" content="周的小站">
<meta property="og:url" content="http://chzhou.cc/index.html">
<meta property="og:site_name" content="周的小站">
<meta property="og:description" content="周的小站">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="周的小站">
<meta name="twitter:description" content="周的小站">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://chzhou.cc/">





  <title>周的小站</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">周的小站</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-resume">
          <a href="/resume/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            简历
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://chzhou.cc/2019/01/01/tvm_sgx_doc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/tvm_sgx_doc/" itemprop="url">TVM_SGX</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-01T22:19:26+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TVM-SGX"><a href="#TVM-SGX" class="headerlink" title="TVM_SGX"></a>TVM_SGX</h1><blockquote>
<p>文档分为两部分，第一部分为TVM自身及SGX属性的编译，第二部分为SGX APP的编译</p>
</blockquote>
<h2 id="TVM-编译"><a href="#TVM-编译" class="headerlink" title="TVM 编译"></a>TVM 编译</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /mnt</span><br><span class="line">mkdir build &amp;&amp; cd build</span><br><span class="line">cmake .. -DUSE_LLVM=ON -DUSE_SGX=/opt/sgxsdk -DRUST_SGX_SDK=/opt/rust-sgx-sdk</span><br><span class="line">make -j4</span><br></pre></td></tr></table></figure>
<p>根据<a href="https://github.com/dmlc/tvm/tree/master/apps/sgx" target="_blank" rel="noopener">文档</a>，在启动好Docker后，进行编译。这里的编译是先由<code>CMakeLists.txt</code>生成<code>Makefile</code>，再进行编译。</p>
<ul>
<li><p>CMakeLists.txt</p>
<p>在TVM主目录下（即/mnt)下，有总的CMakeLists.txt，其中关键语句为：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tvm_option(USE_SGX <span class="string">"Build with SGX"</span> <span class="keyword">OFF</span>)</span><br></pre></td></tr></table></figure>
<p>在这里开启TVM编译时的SGX选项。</p>
</li>
<li><p>在 /mnt/cmake/modules/SGX.cmake里，对SGX的部分进行编译</p>
<p>其中关键语句是在这里，使用SGX SDK里面的sgx_edger8r对<code>tvm.edl</code>进行解析，生成<code>tvm_t.c/h</code>和<code>tvm_u.c/h</code> </p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">add_custom_command</span>(</span><br><span class="line">    OUTPUT <span class="variable">$&#123;_tvm_u_h&#125;</span></span><br><span class="line">    <span class="keyword">COMMAND</span> <span class="variable">$&#123;USE_SGX&#125;</span>/bin/x64/sgx_edger8r --untrusted</span><br><span class="line">      --untrusted --untrusted-dir <span class="variable">$&#123;_sgx_src&#125;</span>/untrusted</span><br><span class="line">      --trusted --trusted-dir <span class="variable">$&#123;_sgx_src&#125;</span>/trusted</span><br><span class="line">      --search-path <span class="variable">$&#123;USE_SGX&#125;</span>/<span class="keyword">include</span> --search-path <span class="variable">$&#123;RUST_SGX_SDK&#125;</span>/edl</span><br><span class="line">      <span class="variable">$&#123;_tvm_edl&#125;</span></span><br><span class="line">    <span class="keyword">COMMAND</span> sed -i <span class="string">"4i '#include &lt;tvm/runtime/c_runtime_api.h&gt;'"</span> <span class="variable">$&#123;_tvm_u_h&#125;</span></span><br><span class="line">    <span class="keyword">COMMAND</span> sed -i <span class="string">"4i '#include &lt;tvm/runtime/c_runtime_api.h&gt;'"</span> <span class="variable">$&#123;_tvm_t_h&#125;</span></span><br><span class="line">    DEPENDS <span class="variable">$&#123;_tvm_edl&#125;</span></span><br><span class="line">  )</span><br></pre></td></tr></table></figure>
</li>
<li><p>TVM本身带有的SGX的代码在 /mnt/src/runtime/sgx/下。在此不详述</p>
</li>
</ul>
<h2 id="SGX-APP编译"><a href="#SGX-APP编译" class="headerlink" title="SGX APP编译"></a>SGX APP编译</h2><p>总的来说，SGX APP的编译分为两部分，一部分为tvm model的编译，另一部分则是enclave的编译。其中该部分的目录在 /mnt/apps/sgx</p>
<ul>
<li><p>首先安装依赖库</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -e python -e topi/python -e nnvm/python</span><br></pre></td></tr></table></figure>
</li>
<li><p>外部程序调用enclave的时候都是引用的<code>enclave.signed.so</code>，所以在看Makefile的时候主要盯着<code>enclave.signed.so</code>的产生流程。</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line"> subgraph TVM_model Part</span><br><span class="line"> J(build_model.py)--&gt;I </span><br><span class="line"> I(model.bc)--&gt;H </span><br><span class="line"> H(model.o)--&gt;F</span><br><span class="line"> end</span><br><span class="line"> subgraph SGX Part</span><br><span class="line"> G(xargo build --target x86_64-unknown-linux-sgx)</span><br><span class="line"> end</span><br><span class="line"> G(xargo build --target x86_64-unknown-linux-sgx)--&gt;|使用./src/lib.rs|E</span><br><span class="line"></span><br><span class="line"> F(libmodel.a)--&gt;E </span><br><span class="line"> E(libmodel_enclave.a)--&gt;|复制为|C </span><br><span class="line"> D(libtvm_t.a)--&gt;B </span><br><span class="line"> C(libenclave.a)--&gt;B </span><br><span class="line"> B(enclave.so)--&gt;|signing|A[enclave.signed.so]</span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>libtvm_t.a</code>哪里来的？</p>
<p>经过实验（注释掉以下代码则不会产生<code>libtvm_t.a</code>)，是在 /mnt/cmake/modules/SGX.cmake 里产生的，代码为：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#build trusted library</span></span><br><span class="line"><span class="keyword">set_source_files_properties</span>(<span class="variable">$&#123;_tvm_t_c&#125;</span> PROPERTIES GENERATED <span class="keyword">TRUE</span>)</span><br><span class="line"><span class="keyword">add_library</span>(tvm_t STATIC <span class="variable">$&#123;_tvm_t_c&#125;</span>)</span><br><span class="line"><span class="keyword">add_dependencies</span>(tvm_t sgx_edl)</span><br><span class="line"><span class="keyword">target_include_directories</span>(tvm_t PUBLIC <span class="variable">$&#123;USE_SGX&#125;</span>/<span class="keyword">include</span> <span class="variable">$&#123;USE_SGX&#125;</span>/<span class="keyword">include</span>/tlibc)</span><br></pre></td></tr></table></figure>
<p>在根目录下的CMakeLists.txt引用该库为：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(<span class="keyword">NOT</span> USE_SGX <span class="keyword">STREQUAL</span> <span class="string">"OFF"</span>)</span><br><span class="line">  <span class="keyword">add_dependencies</span>(tvm sgx_edl)</span><br><span class="line">  <span class="keyword">add_dependencies</span>(tvm_runtime sgx_edl tvm_t)</span><br><span class="line">  <span class="keyword">install</span>(TARGETS tvm_t ARCHIVE DESTINATION lib<span class="variable">$&#123;LIB_SUFFIX&#125;</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br></pre></td></tr></table></figure>
<p>同时，对<code>libtvm_t.a</code>使用<code>objdump</code>命令，输出为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> objdump -f libtvm_t.a:</span><br><span class="line"></span><br><span class="line">In archive libtvm_t.a:</span><br><span class="line"></span><br><span class="line">tvm_t.c.o:     file format elf64-x86-64</span><br><span class="line">architecture: i386:x86-64, flags 0x00000011:</span><br><span class="line">HAS_RELOC, HAS_SYMS</span><br><span class="line">start address 0x0000000000000000</span><br></pre></td></tr></table></figure>
<p>可从中具体得知archive的是tvm_t.c.o</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://chzhou.cc/2018/10/12/Spark中SVM doc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/12/Spark中SVM doc/" itemprop="url">Spark中SVM doc</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-12T21:44:22+08:00">
                2018-10-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Spark中SVM分析"><a href="#Spark中SVM分析" class="headerlink" title="Spark中SVM分析"></a>Spark中SVM分析</h1><p>mllib中的svm只实现了线性二分类，没有非线性（核函数），也没有多分类和回归。</p>
<p>其中在初始化的时候，选取的是SGD(stochastic gradient descent)算法，在该算法运行的过程中，体现了spark的分布式运行。</p>
<h2 id="MlLib中SVM实现"><a href="#MlLib中SVM实现" class="headerlink" title="MlLib中SVM实现"></a>MlLib中SVM实现</h2><p>以下是svm的类的关系图（网上download的）：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/967544-bf2bb84db9564edf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/620/format/webp" alt="SVM"></p>
<h3 id="一-程序入口"><a href="#一-程序入口" class="headerlink" title="一. 程序入口"></a>一. 程序入口</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVMWithSGD</span> <span class="title">private</span> (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    private var stepSize: <span class="type">Double</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var numIterations: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var regParam: <span class="type">Double</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var miniBatchFraction: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">GeneralizedLinearAlgorithm</span>[<span class="type">SVMModel</span>] <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 定义了损失函数和优化函数</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> gradient = <span class="keyword">new</span> <span class="type">HingeGradient</span>()</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> updater = <span class="keyword">new</span> <span class="type">SquaredL2Updater</span>()</span><br><span class="line">  <span class="meta">@Since</span>(<span class="string">"0.8.0"</span>)</span><br><span class="line">  <span class="comment">// new了一个梯度下降的类，命名为optimizer</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> optimizer = <span class="keyword">new</span> <span class="type">GradientDescent</span>(gradient, updater)</span><br><span class="line">    .setStepSize(stepSize)</span><br><span class="line">    .setNumIterations(numIterations)</span><br><span class="line">    .setRegParam(regParam)</span><br><span class="line">    .setMiniBatchFraction(miniBatchFraction)</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span> <span class="keyword">val</span> validators = <span class="type">List</span>(<span class="type">DataValidators</span>.binaryLabelValidator)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Construct a SVM object with default parameters: &#123;stepSize: 1.0, numIterations: 100,</span></span><br><span class="line"><span class="comment">   * regParm: 0.01, miniBatchFraction: 1.0&#125;.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="meta">@Since</span>(<span class="string">"0.8.0"</span>)</span><br><span class="line">  <span class="comment">// 默认参数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>() = <span class="keyword">this</span>(<span class="number">1.0</span>, <span class="number">100</span>, <span class="number">0.01</span>, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">createModel</span></span>(weights: <span class="type">Vector</span>, intercept: <span class="type">Double</span>) = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">SVMModel</span>(weights, intercept)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>SVMWithSGD</code>里面实现了SVM模型基本的一些元素，包括</p>
<ol>
<li>继承了<code>GeneralizedLinearAlgorithm</code></li>
<li>定义了损失函数<code>HingeGradient()</code>，命名为 gradient</li>
<li>定义了L2正则化<code>SquaredL2Updater()</code>，命名为updater</li>
<li>以上面两种作为参数，new一个<code>GradientDescent()</code>，其接收的参数有两个，一个是梯度计算的损失函数，一个是优化函数，最后命名为optimizer</li>
<li>其他就是定义一些默认参数</li>
</ol>
<p>之后在接下来的<code>train()</code>函数里，调用<code>run()</code>进行模型运算。这里的<code>run()</code>继承自<code>GeneralizedLinearAlgorithm</code>类。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span></span>(</span><br><span class="line">    input: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>],</span><br><span class="line">    numIterations: <span class="type">Int</span>,</span><br><span class="line">    stepSize: <span class="type">Double</span>,</span><br><span class="line">    regParam: <span class="type">Double</span>,</span><br><span class="line">    miniBatchFraction: <span class="type">Double</span>,</span><br><span class="line">    initialWeights: <span class="type">Vector</span>): <span class="type">SVMModel</span> = &#123;</span><br><span class="line">    <span class="comment">// new了一个SVMWithSGD类，然后调用run()</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">SVMWithSGD</span>(stepSize, numIterations, regParam, miniBatchFraction)</span><br><span class="line">    .run(input, initialWeights)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="二-运行过程"><a href="#二-运行过程" class="headerlink" title="二. 运行过程"></a>二. 运行过程</h3><p><code>run()</code>函数在GeneralizedLinearAlgorithm.scala文件里。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(input: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>], initialWeights: <span class="type">Vector</span>): <span class="type">M</span> = &#123;</span><br><span class="line">    <span class="comment">// 省去一些初始化和为了计算所进行的优化过程</span></span><br><span class="line">    <span class="comment">// 之前定义好的optimizer调用optimize()函数</span></span><br><span class="line">    <span class="keyword">val</span> weightsWithIntercept = optimizer.optimize(data, initialWeightsWithIntercept)</span><br><span class="line">    <span class="comment">// 其他的一些过程</span></span><br></pre></td></tr></table></figure>
<p>在<code>run()</code>函数里，最关键的计算过程是在上一句，即由<code>new GradientDescent(gradient, updater)</code>生成的optimizer调用其<code>optimize()</code>函数进行优化。gradient是<code>HingeGradient()</code>函数，updater是<code>SquaredL2Updater()</code>。</p>
<h3 id="三-优化过程"><a href="#三-优化过程" class="headerlink" title="三. 优化过程"></a>三. 优化过程</h3><p>在GradientDescent.scala中，定义了<code>optimize()</code>函数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span></span>(data: <span class="type">RDD</span>[(<span class="type">Double</span>, <span class="type">Vector</span>)], initialWeights: <span class="type">Vector</span>): <span class="type">Vector</span> = &#123;</span><br><span class="line">    <span class="comment">// 调用runMiniBatchSGD()函数</span></span><br><span class="line">    <span class="keyword">val</span> (weights, _) = <span class="type">GradientDescent</span>.runMiniBatchSGD(</span><br><span class="line">      data,</span><br><span class="line">      gradient,</span><br><span class="line">      updater,</span><br><span class="line">      stepSize,</span><br><span class="line">      numIterations,</span><br><span class="line">      regParam,</span><br><span class="line">      miniBatchFraction,</span><br><span class="line">      initialWeights,</span><br><span class="line">      convergenceTol)</span><br><span class="line">    weights</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>可以看出运行的是<code>runMiniBatchSGD()</code>函数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runMiniBatchSGD</span></span>(</span><br><span class="line">    data: <span class="type">RDD</span>[(<span class="type">Double</span>, <span class="type">Vector</span>)],</span><br><span class="line">    gradient: <span class="type">Gradient</span>,</span><br><span class="line">    updater: <span class="type">Updater</span>,</span><br><span class="line">    stepSize: <span class="type">Double</span>,</span><br><span class="line">    numIterations: <span class="type">Int</span>,</span><br><span class="line">    regParam: <span class="type">Double</span>,</span><br><span class="line">    miniBatchFraction: <span class="type">Double</span>,</span><br><span class="line">    initialWeights: <span class="type">Vector</span>,</span><br><span class="line">    convergenceTol: <span class="type">Double</span>): (<span class="type">Vector</span>, <span class="type">Array</span>[<span class="type">Double</span>]) = &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 省去计算的一些过程</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (!converged &amp;&amp; i &lt;= numIterations) &#123;</span><br><span class="line">        <span class="comment">// 将weights广播出去</span></span><br><span class="line">        <span class="keyword">val</span> bcWeights = data.context.broadcast(weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 以下是源码中的注释</span></span><br><span class="line">        <span class="comment">// Sample a subset (fraction miniBatchFraction) of the total data</span></span><br><span class="line">        <span class="comment">// compute and sum up the subgradients on this subset (this is one map-reduce)</span></span><br><span class="line">        <span class="keyword">val</span> (gradientSum, lossSum, miniBatchSize) = data.sample(<span class="literal">false</span>, miniBatchFraction, <span class="number">42</span> + i).treeAggregate((<span class="type">BDV</span>.zeros[<span class="type">Double</span>](n), <span class="number">0.0</span>, <span class="number">0</span>L))(</span><br><span class="line">            seqOp = (c, v) =&gt; &#123;</span><br><span class="line">                <span class="comment">// c: (grad, loss, count), v: (label, features)</span></span><br><span class="line">                <span class="keyword">val</span> l = gradient.compute(v._2, v._1, bcWeights.value, <span class="type">Vectors</span>.fromBreeze(c._1))</span><br><span class="line">                (c._1, c._2 + l, c._3 + <span class="number">1</span>)</span><br><span class="line">            &#125;,</span><br><span class="line">            combOp = (c1, c2) =&gt; &#123;</span><br><span class="line">                <span class="comment">// c: (grad, loss, count)</span></span><br><span class="line">                (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)</span><br><span class="line">            &#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 销毁广播变量weights</span></span><br><span class="line">        bcWeights.destroy(blocking = <span class="literal">false</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 下面是计算完成后进行整理和输出log的一些语句</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这个while循环计算梯度的时候，体现出spark分布式计算。</p>
<ol>
<li>先是由data所在的sc进行广播，将weights以广播变量的形式存入各个机器的缓存中。</li>
<li>data为rdd格式，<ol>
<li>调用<code>sample()</code>从每个partition中抽一些sample出来，第一个参数为<code>false</code> 意思为不放回的抽出，此时的各个sample仍在各自的partition中</li>
<li>调用<code>treeAggregate</code>函数对每个partition中的数据进行运算，最后在driver端进行汇总。<code>treeAggregate</code>函数里面有<code>seqOp</code> 和<code>combOp</code> 两个函数，其中<code>seqOp</code>定义了在每个partition中元素的操作，<code>combOp</code>定义了各个partition中元素进行aggregate时的规则，最后在driver端进行汇总计算。</li>
</ol>
</li>
</ol>
<p>在这个里面的<code>treeAggregate</code>函数进行了类似于KMeans中<code>reduceByKey()</code>和<code>collectAsMap()</code>的两步操作。</p>
<h3 id="四-附上HingeGradient-和SquaredL2Updater-的源码"><a href="#四-附上HingeGradient-和SquaredL2Updater-的源码" class="headerlink" title="四. 附上HingeGradient()和SquaredL2Updater()的源码"></a>四. 附上HingeGradient()和SquaredL2Updater()的源码</h3><p><code>HingeGradient()</code>的<code>compute()</code>函数:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HingeGradient</span> <span class="keyword">extends</span> <span class="title">Gradient</span> </span>&#123;</span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</span><br><span class="line">      data: <span class="type">Vector</span>,</span><br><span class="line">      label: <span class="type">Double</span>,</span><br><span class="line">      weights: <span class="type">Vector</span>,</span><br><span class="line">      cumGradient: <span class="type">Vector</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> dotProduct = dot(data, weights)</span><br><span class="line">    <span class="comment">// Our loss function with &#123;0, 1&#125; labels is max(0, 1 - (2y - 1) (f_w(x)))</span></span><br><span class="line">    <span class="comment">// Therefore the gradient is -(2y - 1)*x</span></span><br><span class="line">    <span class="keyword">val</span> labelScaled = <span class="number">2</span> * label - <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="number">1.0</span> &gt; labelScaled * dotProduct) &#123;</span><br><span class="line">      axpy(-labelScaled, data, cumGradient)</span><br><span class="line">      <span class="number">1.0</span> - labelScaled * dotProduct</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="number">0.0</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>SquaredL2Updater()</code>:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SquaredL2Updater</span> <span class="keyword">extends</span> <span class="title">Updater</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</span><br><span class="line">      weightsOld: <span class="type">Vector</span>,</span><br><span class="line">      gradient: <span class="type">Vector</span>,</span><br><span class="line">      stepSize: <span class="type">Double</span>,</span><br><span class="line">      iter: <span class="type">Int</span>,</span><br><span class="line">      regParam: <span class="type">Double</span>): (<span class="type">Vector</span>, <span class="type">Double</span>) = &#123;</span><br><span class="line">    <span class="comment">// add up both updates from the gradient of the loss (= step) as well as</span></span><br><span class="line">    <span class="comment">// the gradient of the regularizer (= regParam * weightsOld)</span></span><br><span class="line">    <span class="comment">// w' = w - thisIterStepSize * (gradient + regParam * w)</span></span><br><span class="line">    <span class="comment">// w' = (1 - thisIterStepSize * regParam) * w - thisIterStepSize * gradient</span></span><br><span class="line">    <span class="keyword">val</span> thisIterStepSize = stepSize / math.sqrt(iter)</span><br><span class="line">    <span class="keyword">val</span> brzWeights: <span class="type">BV</span>[<span class="type">Double</span>] = weightsOld.asBreeze.toDenseVector</span><br><span class="line">    brzWeights :*= (<span class="number">1.0</span> - thisIterStepSize * regParam)</span><br><span class="line">    brzAxpy(-thisIterStepSize, gradient.asBreeze, brzWeights)</span><br><span class="line">    <span class="keyword">val</span> norm = brzNorm(brzWeights, <span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">    (<span class="type">Vectors</span>.fromBreeze(brzWeights), <span class="number">0.5</span> * regParam * norm * norm)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://chzhou.cc/2018/10/11/Spark中K-Means doc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/11/Spark中K-Means doc/" itemprop="url">Spark中KMeans doc</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-11T09:17:12+08:00">
                2018-10-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a>KMeans</h1><h2 id="例子代码"><a href="#例子代码" class="headerlink" title="例子代码"></a>例子代码</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//原文链接：http://dblab.xmu.edu.cn/blog/1454-2/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.&#123;<span class="type">KMeans</span>,<span class="type">KMeansModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vectors</span> <span class="comment">// 原文是Vectors，但是出错，经过搜索发现是Vector :http://lxw1234.com/archives/2016/01/605.htm</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._   <span class="comment">//开启隐式转换</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">model_instance</span> (<span class="params">features: <span class="type">Vector</span></span>)    <span class="title">//开启隐式转换和创建这个model_instance（好像）是调用</span> .<span class="title">toDF</span>(<span class="params"></span>) <span class="title">的必要条件</span></span></span><br><span class="line"><span class="class"> </span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rawData</span> </span>= sc.textFile(<span class="string">"hdfs://lotus02:9000/user/chzhou/data.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = rawData.map(line =&gt;</span><br><span class="line">    &#123; model_instance( <span class="type">Vectors</span>.dense(line.split(<span class="string">","</span>).filter(p =&gt; p.matches(<span class="string">"\\d*(\\.?)\\d*"</span>))    <span class="comment">// '\\d'为匹配数字</span></span><br><span class="line">        .map(_.toDouble)) )&#125;).toDF()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> kmeansmodel = <span class="keyword">new</span> <span class="type">KMeans</span>().</span><br><span class="line">    setK(<span class="number">3</span>).</span><br><span class="line">    setFeaturesCol(<span class="string">"features"</span>).</span><br><span class="line">    setPredictionCol(<span class="string">"prediction"</span>).</span><br><span class="line">    fit(df)</span><br></pre></td></tr></table></figure>
<p>数据采用的是<a href="http://dblab.xmu.edu.cn/blog/wp-content/uploads/2017/03/iris.txt" target="_blank" rel="noopener">iris</a>数据，有四个实数值的特征，分别代表花朵四个部位的尺寸，以及该样本对应鸢尾花的亚种类型（共有3种亚种类型）。</p>
<h2 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h2><p>对程序在spark-shell中按条输入，在最后一步 KMeans.fit(df) 的时候整个程序转换才开始进行。</p>
<p>整个程序总共有9个job，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/11/5bbea460c3933.png" alt="job.png"></p>
<p>总共分为13个stage，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/11/5bbea4852f2a3.png" alt="stage.png"></p>
<p>虽然有13个，但是总共可以分为4个阶段，分别对应kmeans实现的4个阶段</p>
<h2 id="spark中kmeans实现"><a href="#spark中kmeans实现" class="headerlink" title="spark中kmeans实现"></a>spark中kmeans实现</h2><p>本次导入的库为spark中ml包。实际上ml中的kmeans只是对mllib中kmeans的封装，mllib.KMeans的接口是基于RDD的，而ml.KMeans的接口是基于DataFrame的。所以在调用ml.KMeans的fit( )函数后，内部其实是对DataFrame进行转换，转换为RDD形式，再将数据作为输入对mllib.KMeans进行调用从而训练模型，训练完后再返回给ml.KMeans。</p>
<p>mllib.KMeans中，在选取初始点时，实际上默认的算法采用的是KMeans||算法（算法的lineage是：普通kmeans -&gt; kmeans++ -&gt; KMeans||)。其实最主要的不同之处在于选取初始质心的策略上。经典的Kmeans算法的缺点在于需要预先指定k值以及对初始选取的质心比较敏感。为了解决该问题提出了<a href="https://en.wikipedia.org/wiki/K-means++" target="_blank" rel="noopener">kmeans++算法</a>，对于质心的选择进行了改变，但是问题在于算法必须顺序执行，无法并行扩展。针对此问题又提出了KMeans||算法，<a href="http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf" target="_blank" rel="noopener">论文在这里</a>。</p>
<h3 id="ml-KMeans"><a href="#ml-KMeans" class="headerlink" title="ml.KMeans"></a>ml.KMeans</h3><p>在ml中的kmens中，先将mllib中的kmeans包进行引入，同时为了避免和ml中原有的kmeans类混淆，重新命名为MLlibKMeans，MLlibKMeansModel。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.clustering.&#123;<span class="type">DistanceMeasure</span>, <span class="type">KMeans</span> =&gt; <span class="type">MLlibKMeans</span>, <span class="type">KMeansModel</span> =&gt; <span class="type">MLlibKMeansModel</span>&#125;</span><br></pre></td></tr></table></figure>
<p>函数的训练入口是fit()函数，从这里开始，并且先将DataFrame转化为rdd形式</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">fit</span></span>(dataset: <span class="type">Dataset</span>[_]): <span class="type">KMeansModel</span> = instrumented &#123; instr =&gt;</span><br><span class="line">    transformSchema(dataset.schema, logging = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> handlePersistence = dataset.storageLevel == <span class="type">StorageLevel</span>.<span class="type">NONE</span></span><br><span class="line">    <span class="keyword">val</span> instances = <span class="type">DatasetUtils</span>.columnToOldVector(dataset, getFeaturesCol)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (handlePersistence) &#123;</span><br><span class="line">      instances.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>)</span><br><span class="line">    &#125;   <span class="comment">//将rdd的存储等级设置为StorageLevel.MEMORY_AND_DISK</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 不知道在干什么。。。是调用的ml库自己的instrument方法（没影响）</span></span><br><span class="line">    instr.logPipelineStage(<span class="keyword">this</span>)</span><br><span class="line">    instr.logDataset(dataset)</span><br><span class="line">    instr.logParams(<span class="keyword">this</span>, featuresCol, predictionCol, k, initMode, initSteps, distanceMeasure,maxIter, seed, tol)</span><br></pre></td></tr></table></figure>
<p>然后将ml中自己的kmeans模型参数送入mllib的模型中，命名为algo.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> algo = <span class="keyword">new</span> <span class="type">MLlibKMeans</span>()</span><br><span class="line">      .setK($(k))</span><br><span class="line">      .setInitializationMode($(initMode))</span><br><span class="line">      .setInitializationSteps($(initSteps))</span><br><span class="line">      .setMaxIterations($(maxIter))</span><br><span class="line">      .setSeed($(seed))</span><br><span class="line">      .setEpsilon($(tol))</span><br><span class="line">      .setDistanceMeasure($(distanceMeasure))</span><br></pre></td></tr></table></figure>
<p>调用mllib中kmeans的run()函数，进行运算。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> parentModel = algo.run(instances, <span class="type">Option</span>(instr))</span><br></pre></td></tr></table></figure>
<p>此时进入mllib中的kmeans模型实现函数。</p>
<h3 id="mllib-KMeans"><a href="#mllib-KMeans" class="headerlink" title="mllib.KMeans"></a>mllib.KMeans</h3><p>在mllib.kmeans中，执行的大致顺序如下：</p>
<ol>
<li>将rdd中的point变为（point，norm）形式。其中point的存储形式是vector，norm是二范数，即point的向量模。有了模之后方便以后计算各个点之间的距离。</li>
<li>用initRandom或者initKMeansParallel方法进行对初始中心点的选择。其中默认的方式是initKmeansParallel方法，也就是KMeans||算法</li>
<li>在初始的中心点选择好后，进行对模型的收敛计算，直到达到允许的误差值内或者达到最大迭代计算次数。</li>
<li>返回模型</li>
</ol>
<h4 id="一-rdd中point转换"><a href="#一-rdd中point转换" class="headerlink" title="一. rdd中point转换"></a>一. rdd中point转换</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(</span><br><span class="line">      data: <span class="type">RDD</span>[<span class="type">Vector</span>],</span><br><span class="line">      instr: <span class="type">Option</span>[<span class="type">Instrumentation</span>]): <span class="type">KMeansModel</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (data.getStorageLevel == <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">      logWarning(<span class="string">"The input data is not directly cached, which may hurt performance if its"</span></span><br><span class="line">        + <span class="string">" parent RDDs are also uncached."</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算模并且缓存下来</span></span><br><span class="line">    <span class="keyword">val</span> norms = data.map(<span class="type">Vectors</span>.norm(_, <span class="number">2.0</span>))</span><br><span class="line">    norms.persist()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 将模与原来的rdd中的点zip在一起</span></span><br><span class="line">    <span class="keyword">val</span> zippedData = data.zip(norms).map &#123; <span class="keyword">case</span> (v, norm) =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">VectorWithNorm</span>(v, norm)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 调用runAlgorithm()进行计算</span></span><br><span class="line">    <span class="keyword">val</span> model = runAlgorithm(zippedData, instr)</span><br></pre></td></tr></table></figure>
<h4 id="二-初始化中心点"><a href="#二-初始化中心点" class="headerlink" title="二. 初始化中心点"></a>二. 初始化中心点</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runAlgorithm</span></span>(</span><br><span class="line">      data: <span class="type">RDD</span>[<span class="type">VectorWithNorm</span>],</span><br><span class="line">      instr: <span class="type">Option</span>[<span class="type">Instrumentation</span>]): <span class="type">KMeansModel</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = data.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> initStartTime = <span class="type">System</span>.nanoTime()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> distanceMeasureInstance = <span class="type">DistanceMeasure</span>.decodeFromString(<span class="keyword">this</span>.distanceMeasure)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//在这里可以看出，除非指明初始化的方法为initRandom，否则默认为initKmeansParallel</span></span><br><span class="line">    <span class="keyword">val</span> centers = initialModel <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(kMeansCenters) =&gt;</span><br><span class="line">        kMeansCenters.clusterCenters.map(<span class="keyword">new</span> <span class="type">VectorWithNorm</span>(_))</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="keyword">if</span> (initializationMode == <span class="type">KMeans</span>.<span class="type">RANDOM</span>) &#123;</span><br><span class="line">          initRandom(data)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          initKMeansParallel(data, distanceMeasureInstance)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>下面进入initKMeansParallel方法:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[clustering] <span class="function"><span class="keyword">def</span> <span class="title">initKMeansParallel</span></span>(data: <span class="type">RDD</span>[<span class="type">VectorWithNorm</span>],</span><br><span class="line">      distanceMeasureInstance: <span class="type">DistanceMeasure</span>): <span class="type">Array</span>[<span class="type">VectorWithNorm</span>] = &#123;</span><br><span class="line">    <span class="comment">// 初始化costs</span></span><br><span class="line">    <span class="keyword">var</span> costs = data.map(_ =&gt; <span class="type">Double</span>.<span class="type">PositiveInfinity</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 在rdd中随机选一个点</span></span><br><span class="line">    <span class="keyword">val</span> seed = <span class="keyword">new</span> <span class="type">XORShiftRandom</span>(<span class="keyword">this</span>.seed).nextInt()</span><br><span class="line">    <span class="keyword">val</span> sample = data.takeSample(<span class="literal">false</span>, <span class="number">1</span>, seed)</span><br><span class="line">    </span><br><span class="line">    require(sample.nonEmpty, <span class="string">s"No samples available from <span class="subst">$data</span>"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 将随机选的那一个点作为第一个中心点</span></span><br><span class="line">    <span class="keyword">val</span> centers = <span class="type">ArrayBuffer</span>[<span class="type">VectorWithNorm</span>]()</span><br><span class="line">    <span class="keyword">var</span> newCenters = <span class="type">Seq</span>(sample.head.toDense)</span><br><span class="line">    centers ++= newCenters</span><br></pre></td></tr></table></figure>
<p>takesample此时发生了rdd的计算，这时候的过程对应于stage 0 和 stage 1。</p>
<p>接下来通过多次循环计算，取得所有的初始化中心点。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//用来存储每次产生的中心点，并且是broadcast类型</span></span><br><span class="line"><span class="keyword">val</span> bcNewCentersList = <span class="type">ArrayBuffer</span>[<span class="type">Broadcast</span>[_]]()</span><br><span class="line"><span class="keyword">while</span> (step &lt; initializationSteps) &#123;</span><br><span class="line">    <span class="comment">// 每次把上一次算出来的newCenters广播出去</span></span><br><span class="line">    <span class="keyword">val</span> bcNewCenters = data.context.broadcast(newCenters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 把新算出来的点加到里面</span></span><br><span class="line">    bcNewCentersList += bcNewCenters</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 计算data里面点的cost值</span></span><br><span class="line">    <span class="keyword">val</span> preCosts = costs</span><br><span class="line">    costs = data.zip(preCosts).map &#123; <span class="keyword">case</span> (point, cost) =&gt;</span><br><span class="line">        math.min(distanceMeasureInstance.pointCost(bcNewCenters.value, point), cost)</span><br><span class="line">    &#125;.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>)</span><br><span class="line">    <span class="keyword">val</span> sumCosts = costs.sum()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 将broadcast变量销毁</span></span><br><span class="line">    bcNewCenters.unpersist(blocking = <span class="literal">false</span>)</span><br><span class="line">    preCosts.unpersist(blocking = <span class="literal">false</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 开始选点，每次循环根据距中心点的距离成比例地选取 2 * k 个点</span></span><br><span class="line">    <span class="keyword">val</span> chosen = data.zip(costs).mapPartitionsWithIndex &#123; (index, pointCosts) =&gt; <span class="keyword">val</span> rand = <span class="keyword">new</span> <span class="type">XORShiftRandom</span>(seed ^ (step &lt;&lt; <span class="number">16</span>) ^ index)</span><br><span class="line">   pointCosts.filter &#123; <span class="keyword">case</span> (_, c) =&gt; rand.nextDouble() &lt; <span class="number">2.0</span> * c * k / sumCosts &#125;.map(_._1)</span><br><span class="line">    &#125;.collect()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 把新选择出来地点变为dense格式，命名为newCenters</span></span><br><span class="line">    newCenters = chosen.map(_.toDense)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 把新的center放入到centers里面</span></span><br><span class="line">    centers ++= newCenters</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对循环得到的centers处理一下，先是转换为vector形式，去重，再转换为VectorWithNorm格式。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> distinctCenters = centers.map(_.vector).distinct.map(<span class="keyword">new</span> <span class="type">VectorWithNorm</span>(_))</span><br></pre></td></tr></table></figure>
<p>如果找出来的centers比k多，通过LocalKMeans筛检出k个中心点。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (distinctCenters.size &lt;= k) &#123;</span><br><span class="line">      distinctCenters.toArray</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> bcCenters = data.context.broadcast(distinctCenters)</span><br><span class="line">      <span class="keyword">val</span> countMap = data</span><br><span class="line">        .map(distanceMeasureInstance.findClosest(bcCenters.value, _)._1)</span><br><span class="line">        .countByValue()</span><br><span class="line"></span><br><span class="line">      bcCenters.destroy(blocking = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> myWeights = distinctCenters.indices.map(countMap.getOrElse(_, <span class="number">0</span>L).toDouble).toArray</span><br><span class="line">      <span class="type">LocalKMeans</span>.kMeansPlusPlus(<span class="number">0</span>, distinctCenters.toArray, myWeights, k, <span class="number">30</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="三-对模型进行收敛计算"><a href="#三-对模型进行收敛计算" class="headerlink" title="三. 对模型进行收敛计算"></a>三. 对模型进行收敛计算</h4><p>初始化选点做完后，将中心点存入到centers中，进行收敛计算，找出最后收敛的点的集合。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (iteration &lt; maxIterations &amp;&amp; !converged) &#123;</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 用到了累加器，用来记录计算过程中整体的cost值。该变量只能通过关联操作进行“加”运算，并且在各个worker上进行同步</span></span><br><span class="line">      <span class="keyword">val</span> costAccum = sc.doubleAccumulator</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 将中心点的集合通过broadcast广播出去，在每个worker上都有一份该缓存，并且为只读</span></span><br><span class="line">      <span class="keyword">val</span> bcCenters = sc.broadcast(centers)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 对rdd中的各个partition做操作，分别找见各个partition中点的聚类中心</span></span><br><span class="line">      <span class="keyword">val</span> newCenters = data.mapPartitions &#123; points =&gt;</span><br><span class="line">        <span class="comment">// 读取中心点的数值和相关维度</span></span><br><span class="line">        <span class="keyword">val</span> thisCenters = bcCenters.value</span><br><span class="line">        <span class="keyword">val</span> dims = thisCenters.head.vector.size</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化数组，第一个sums数组用来存储各个中心点中的全部点的向量和</span></span><br><span class="line">        <span class="keyword">val</span> sums = <span class="type">Array</span>.fill(thisCenters.length)(<span class="type">Vectors</span>.zeros(dims))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 第二个counts用来记录每个中心点的点簇的数量</span></span><br><span class="line">        <span class="keyword">val</span> counts = <span class="type">Array</span>.fill(thisCenters.length)(<span class="number">0</span>L)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//对每个partition中的各个点做以下操作</span></span><br><span class="line">        points.foreach &#123; point =&gt;</span><br><span class="line">          <span class="comment">// 各个点与每个中心点算距离，返回其中距离最小的。其中bestCenter是中心点在centers中的index，cost是两点之间的距离</span></span><br><span class="line">          <span class="keyword">val</span> (bestCenter, cost) = distanceMeasureInstance.findClosest(thisCenters, point)</span><br><span class="line">          </span><br><span class="line">          <span class="comment">// 在全局上将cost进行累加</span></span><br><span class="line">          costAccum.add(cost)</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 把这个点与所属中心点的向量和存储到sums里面 </span></span><br><span class="line">          distanceMeasureInstance.updateClusterSum(point, sums(bestCenter))</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 该中心点下的点的个数加1</span></span><br><span class="line">          counts(bestCenter) += <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 在对每个点做完以上操作后，每个partition中的点对应的中心点及其cost也都计算出来了</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 对counts中大于0的进行筛选，返回index （等于0说明该中心点下没有对应的点，自然要删掉）</span></span><br><span class="line">        <span class="comment">// 返回的index形成了一个list，调用map语句对list中的每个index做一层包裹，形成 （index, (sum(index), counts(index)) 的形式</span></span><br><span class="line">        <span class="comment">// 因为mappartition要返回iterator类型，所以在后面加一个iterator</span></span><br><span class="line">        counts.indices.filter(counts(_) &gt; <span class="number">0</span>).map(j =&gt; (j, (sums(j), counts(j)))).iterator</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 下面的reduceByKey对同一个index（也就是同一个中心点）中的数据进行聚合 （因为数据分散在各个worker上）</span></span><br><span class="line">      &#125;.reduceByKey &#123; <span class="keyword">case</span> ((sum1, count1), (sum2, count2)) =&gt;</span><br><span class="line">        <span class="comment">// 对于相同的index，其中的值做以下操作</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将sum2累加到sum1中</span></span><br><span class="line">        axpy(<span class="number">1.0</span>, sum2, sum1)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将count2累加到count1上</span></span><br><span class="line">        (sum1, count1 + count2)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// collectAsMap()将所有聚合后的数据送入到driver端，让driver进行下一步操作</span></span><br><span class="line">        <span class="comment">// mapValues只对数据的value字段进行map操作，从(sum, count)信息中重新计算中心点  （数据是k-v，形式为（index, (sum(index), counts(index))）</span></span><br><span class="line">      &#125;.collectAsMap().mapValues &#123; <span class="keyword">case</span> (sum, count) =&gt;</span><br><span class="line">        distanceMeasureInstance.centroid(sum, count)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 以上做完后就把新的中心点存入到了newCenters中</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 销毁掉之前centers这个广播变量</span></span><br><span class="line">      bcCenters.destroy(blocking = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 重新进行计算，看看有没有收敛。要是没有收敛了就继续算</span></span><br><span class="line">      converged = <span class="literal">true</span></span><br><span class="line">      newCenters.foreach &#123; <span class="keyword">case</span> (j, newCenter) =&gt;</span><br><span class="line">        <span class="keyword">if</span> (converged &amp;&amp;</span><br><span class="line">          !distanceMeasureInstance.isCenterConverged(centers(j), newCenter, epsilon)) &#123;</span><br><span class="line">          converged = <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line">        centers(j) = newCenter</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      cost = costAccum.value</span><br><span class="line">      iteration += <span class="number">1</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>剩下的代码就是输出一些log信息，最后返回kmeans模型。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li><p>分布式的计算体现在哪里？</p>
<p>对模型进行收敛计算中，体现分布式的地方有两点：</p>
<ul>
<li><p>循环开始时：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> costAccum = sc.doubleAccumulator</span><br><span class="line"><span class="keyword">val</span> bcCenters = sc.broadcast(centers)</span><br></pre></td></tr></table></figure>
<p>第一个costAccum是spark的累加器的使用，它在各个worker中间进行同步。它的值仅能够通过“加”改变，所以常常被用来计数，并且只有driver能够读取它的值。在kmeans中用来记录全局的cost值。</p>
<p>第二个bcCenters是广播变量。driver将中心点的集合通过broadcast广播出去，于是bcCenters在每个worker上都有一份缓存，并且为只读变量。</p>
</li>
<li><p>对中心点进行reduceByKey操作后，调用collectAsMap。这个collectAsMap的文档解释是:”Return the key-value pairs in this RDD to the <strong>master</strong> as a Map”。也就是说reduceByKey在reducer端做完后，将数据通过调用collectAsMap送入到driver端中，让driver进行接下来的运算。</p>
</li>
</ul>
</li>
<li><p>在进行reduceByKey操作时，reducer端有几个？和什么有关系？</p>
<p>大概说一下我的理解：</p>
<ol>
<li><p>在mapreduce里，reducer的number是很重要的(显式指定？)。</p>
</li>
<li><p>而在spark中，在reducebykey时会发生shuffle，此时比较重要的是看子阶段rdd的partition个数(因为意味着数据会分在几个partition里面)。如果这个partition个数在reducebykey函数里面没有指定，则取决于partitioner中的partition个数。默认的实现是直接取spark.default.parallelism这个配置项的值作为分区数的，如果没有配置，则以RDD（即map的最后一个RDD）的分区数为准。</p>
<p>所以在reducebykey的时候，是没有reducer端的，而是在各个partition端作sort，数据分散在该例子中的两个partition中。在此之后通过collectAsMap()将数据汇集在driver端，由driver进行之后的操作。</p>
</li>
</ol>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://chzhou.cc/2018/08/31/SGX Batcher‘s sort/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/31/SGX Batcher‘s sort/" itemprop="url">SGX Batcher's sort</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-31T13:30:46+08:00">
                2018-08-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="SGX-enclave"><a href="#SGX-enclave" class="headerlink" title="SGX enclave"></a>SGX enclave</h1><h2 id="一-目标"><a href="#一-目标" class="headerlink" title="一. 目标"></a>一. 目标</h2><p>通过在SGX中实现 batcher‘s sort，进行 enclave runtime 测试</p>
<h2 id="二-难点"><a href="#二-难点" class="headerlink" title="二. 难点"></a>二. 难点</h2><ol>
<li><p>整个 enclave 的程序逻辑是什么？</p>
</li>
<li><p>enclave 如何与 不信任区（uRTS) 的data 做交互？</p>
</li>
<li><p>（疑问）</p>
<p>data进入enclave时应该为加密状态，再由enclave解密。data在 uRTS 中就应为加密状态，那么谁来给data加密？外部函数还是sgx？</p>
<ul>
<li><p>如果是外部函数，因为其在不信任区，有风险</p>
</li>
<li><p>如果是SGX来加密，那么是如何来操作的？</p>
</li>
</ul>
</li>
</ol>
<h2 id="三-流程"><a href="#三-流程" class="headerlink" title="三. 流程"></a>三. 流程</h2><ol>
<li>在SGX内部实现batcher’s sort 算法</li>
<li>准备数据，确定SGX如何读写数据</li>
<li>根据数据修改算法接口</li>
<li>进行测试</li>
</ol>
<h2 id="四-技术点"><a href="#四-技术点" class="headerlink" title="四. 技术点"></a>四. 技术点</h2><ol>
<li><p>batcher sort<br><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Batcher_Odd-Even_Mergesort_for_eight_inputs.svg/356px-Batcher_Odd-Even_Mergesort_for_eight_inputs.svg.png" alt="batcher.jpg"></p>
<p><img src="https://i.loli.net/2018/08/31/5b88cd1ddf834.jpg" alt="batcher.jpg"></p>
</li>
</ol>
<ul>
<li>Batcher排序网络是由一系列Batcher比较器（Batcher’s Comparator）组成的。Batcher比较器是指在两个输入端给定输入x,y，再在两个输出端输出最大值max{x,y}和最小值min{x,y}。</li>
<li>长度为2的倍数。</li>
<li>data-independent</li>
</ul>
<ol start="2">
<li><p>SGX文件结构</p>
<ol>
<li><p>模块</p>
<ul>
<li><p>Untrusted Run-Time System (uRTS) – code that executes outside of the Intel SGX<br>enclave environment and performs functions such as:</p>
<ul>
<li><p>Loading and managing an enclave</p>
</li>
<li><p>Making calls to an enclave and receiving calls from within an enclave</p>
</li>
</ul>
</li>
<li><p>Trusted Run-Time System (tRTS) – code that executes within an Intel SGX enclave envir-<br>  onment and performs functions such as:</p>
<ul>
<li><p>Receiving calls into the enclave and making calls outside of an enclave</p>
</li>
<li><p>Managing the enclave itself</p>
</li>
<li><p>Standard C/C++ libraries and run-time environment</p>
</li>
</ul>
</li>
<li>Edge Routines – functions that may run outside the enclave (untrusted edge routines) or inside the enclave (trusted edge routines) and serve to bind a call from the applic-ation with a function inside the enclave or a call from the enclave with a function in the application</li>
<li>3rd Party Libraries – for the purpose of this document, this is any library that has been tailored to work inside the Intel SGX enclave environment</li>
</ul>
</li>
<li><p>两个术语：</p>
<ul>
<li>ECall：“Enclave Call” a call made into an interface function within the enclave</li>
<li>OCall: “Out Call” a call made from within the enclave to the outside application</li>
</ul>
</li>
<li><p>实际文件结构</p>
<ul>
<li><p>./App </p>
<p>该文件夹存放应用程序中的<strong>不可信</strong>代码部分</p>
<ul>
<li>App.cpp文件：该文件是应用程序中的不可信部分代码，其中包括了创建Enclave及销毁Enclave的代码，也定义了一些相关的返回码供使用者查看Enclave程序的执行状态。其中的main函数是整个项目的入口函数。</li>
</ul>
</li>
<li><p>./Enclave</p>
<p>该文件夹存放应用程序中的可信代码部分和可信与不可信代码接口文件</p>
<ul>
<li>Enclave.config.xml文件：该文件是Enclave的配置文件，定义了Enclave中stack，heap等大小信息</li>
<li>Enclave.cpp文件：该文件是应用程序中的可信部分代码，包括了可信函数的实现</li>
<li>Enclave.edl文件：该文件是Enclave的接口定义文件，定义了不可信代码调用可信代码的ECALL函数接口和可信代码调用不可信代码的OECALL函数接口</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://chzhou.cc/2018/08/21/Enclave文件结构/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/21/Enclave文件结构/" itemprop="url">Enclave文件结构</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-21T09:51:07+08:00">
                2018-08-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Enclave文件结构"><a href="#Enclave文件结构" class="headerlink" title="Enclave文件结构"></a>Enclave文件结构</h1><p>大致总结一下Enclave的程序文件结构，参考的结构是Ubuntu 16.04 Desktop Intel SGX Linux 2.2 Release中的SampleCode文件。</p>
<p>这里以文件中SampleEnclave文件目录为例。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── App</span><br><span class="line">│   ├── App.cpp</span><br><span class="line">│   ├── App.h</span><br><span class="line">│   ├── Edger8rSyntax</span><br><span class="line">│   │   ├── Arrays.cpp</span><br><span class="line">│   │   ├── Functions.cpp</span><br><span class="line">│   │   ├── Pointers.cpp</span><br><span class="line">│   │   └── Types.cpp</span><br><span class="line">│   └── TrustedLibrary</span><br><span class="line">│       ├── Libc.cpp</span><br><span class="line">│       ├── Libcxx.cpp</span><br><span class="line">│       └── Thread.cpp</span><br><span class="line">├── Enclave</span><br><span class="line">│   ├── config.01.xml</span><br><span class="line">│   ├── config.02.xml</span><br><span class="line">│   ├── config.03.xml</span><br><span class="line">│   ├── config.04.xml</span><br><span class="line">│   ├── Edger8rSyntax</span><br><span class="line">│   │   ├── Arrays.cpp</span><br><span class="line">│   │   ├── Arrays.edl</span><br><span class="line">│   │   ├── Functions.cpp</span><br><span class="line">│   │   ├── Functions.edl</span><br><span class="line">│   │   ├── Pointers.cpp</span><br><span class="line">│   │   ├── Pointers.edl</span><br><span class="line">│   │   ├── Types.cpp</span><br><span class="line">│   │   └── Types.edl</span><br><span class="line">│   ├── Enclave.config.xml</span><br><span class="line">│   ├── Enclave.cpp</span><br><span class="line">│   ├── Enclave.edl</span><br><span class="line">│   ├── Enclave.h</span><br><span class="line">│   ├── Enclave.lds</span><br><span class="line">│   ├── Enclave_private.pem</span><br><span class="line">│   └── TrustedLibrary</span><br><span class="line">│       ├── Libc.cpp</span><br><span class="line">│       ├── Libc.edl</span><br><span class="line">│       ├── Libcxx.cpp</span><br><span class="line">│       ├── Libcxx.edl</span><br><span class="line">│       ├── Thread.cpp</span><br><span class="line">│       └── Thread.edl</span><br><span class="line">├── Include</span><br><span class="line">│   └── user_types.h</span><br><span class="line">├── Makefile</span><br><span class="line">└── README.txt</span><br></pre></td></tr></table></figure>
<ol>
<li><p>App文件</p>
<p>该文件夹存放应用程序中的<strong>不可信</strong>代码部分。</p>
<ul>
<li>App.cpp文件：该文件是应用程序中的不可信部分代码，其中包括了创建Enclave及销毁Enclave的代码，也定义了一些相关的返回码供使用者查看Enclave程序的执行状态。其中的main函数是整个项目的入口函数。</li>
<li>App.h文件：该文件是应用程序中的不可信部分代码的头文件，定义了一些宏常量和函数声明。</li>
<li>TrustedLibrary和Edger8rSyntax文件夹：提供函数库和工具</li>
</ul>
</li>
<li><p>Enclave文件夹</p>
<p>该文件夹存放应用程序中的<strong>可信代码</strong>部分和<strong>可信与不可信代码接口</strong>文件</p>
<ul>
<li>Enclave.config.xml文件：该文件是Enclave的配置文件，定义了Enclave的元数据信息</li>
<li>Enclave.cpp文件：该文件是应用程序中的可信部分代码，包括了可信函数的实现</li>
<li>Enclave.h文件：该文件是应用程序中的可信部分代码的头文件，定义了一些宏常量和函数声明</li>
<li>Enclave.edl文件：该文件是Enclave的接口定义文件，定义了不可信代码调用可信代码的ECALL函数接口和可信代码调用不可信代码的OECALL函数接口</li>
<li>Enclave.lds文件：该文件定义了一些Enclave可执行文件信息</li>
<li>Enclave_private.pem文件：该文件是SGX生成的私钥</li>
</ul>
</li>
<li><p>Include文件夹</p>
<p>该文件夹存放被Enclave接口定义文件Enclave.edl使用的头文件，包括一些宏定义。</p>
</li>
<li><p>Makefile文件</p>
<p>该文件是项目的编译文件，定义了项目的编译信息</p>
</li>
</ol>
<p>在编译后，会生成名为 ‘app’ 的可执行文件。</p>
<p>SampleCode文件下的其他文件大同小异，结构都差不多。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://chzhou.cc/2018/08/13/SGX Q&A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/13/SGX Q&A/" itemprop="url">SGX Q&A</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-13T21:14:43+08:00">
                2018-08-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li><p>EPC是什么？物理位置在哪里？</p>
<ul>
<li>EPC:Enclave Page Cache.</li>
<li>The contents of enclaves and the associated data structures are stored in the Enclave Page Cache (EPC), which is a subset of <strong>DRAM</strong>. (which is not on CPU and it is <strong>MAIN MEMORY</strong>)</li>
</ul>
<p><img src="https://insujang.github.io/assets/images/170403/epc.png" alt="EPC and PRM layout"></p>
<p>注：PRM:Processor Reserved Memory</p>
<p>(<a href="https://insujang.github.io/2017-04-03/intel-sgx-protection-mechanism/" target="_blank" rel="noopener">参考链接</a>)</p>
</li>
<li><p>谁对EPC有读取权限？</p>
<p>应用程序由可信部分和不可信部分构成。只有可信函数被调用，才能访问EPC。其他均被阻挡。<a href="https://software.intel.com/zh-cn/sgx/details" target="_blank" rel="noopener">参考链接</a></p>
<p><img src="https://software.intel.com/sites/default/files/managed/6f/ab/runtime-execution.png" alt></p>
</li>
<li><p>现在的EPC是多大？</p>
<ul>
<li>因为EPC在内存中，而内存又被多个其他进程使用，为了不产生冲突，经过Intel分析后将大小设置为定值.<a href="https://software.intel.com/en-us/forums/intel-software-guard-extensions-intel-sgx/topic/737218" target="_blank" rel="noopener">参考链接</a></li>
<li>对于Win：如果OEM支持PRMRR选项（没有查到PRMRR准确定义，大概就是个选项），那么可将大小设置为32 MB, 64 MB or 128 MB。BIOS中默认大小为 128 MB.<a href="https://software.intel.com/en-us/articles/getting-started-with-sgx-sdk-for-windows" target="_blank" rel="noopener">参考链接-官网</a></li>
<li>对于Linux：因为 Linux支持 paging 技术，而Win不支持。所以在Linux中可以突破Win的限制，在所参考的链接里大小最大达到了4G. <a href="https://software.intel.com/en-us/forums/intel-software-guard-extensions-intel-sgx/topic/670322#comment-1878875" target="_blank" rel="noopener">参考链接-时间为16年到17年初</a></li>
<li>在上一条的参考链接里，有条回答进行了总结：“The physical protected memory is limited to the PRMRR size set in BIOS and the max we support at this time is 128MB in Skylake. The reason why you are able to set the heapsize you set is because of the paging support in Linux driver and we don’t have this support in Windows at this time. Similar to how memory is managed in OS, enclave pages are managed similarly.”</li>
</ul>
</li>
<li><p>数据送到EPC中是加密的还是未加密的？</p>
<p>这个问题得到的资料比较混乱，现未有准确答案。只简单罗列搜索到的资料。</p>
<ul>
<li><p><a href="https://software.intel.com/zh-cn/sgx/details" target="_blank" rel="noopener">参考资料1</a></p>
<p>对于enclave（中文为“围圈”），所有进程数据均以明文形式可见；外部访问围圈数据被拒绝</p>
</li>
<li><p><a href="https://software.intel.com/zh-cn/videos/how-to-seal-data-in-intel-sgx" target="_blank" rel="noopener">参考资料2-页面下面文字稿选项第四段</a></p>
<p>该资料未明确提到在enclave中数据是不是加密的。但是提到将数据从enclave到不信任的内存中，需要进行sealing，即进行加密。所以从这个行为推测enclave是未加密的（否则如果是加密的话就不需要sealing了。不知道这样理解对不对）</p>
</li>
<li><p><a href="https://software.intel.com/en-us/forums/intel-software-guard-extensions-intel-sgx/topic/722444" target="_blank" rel="noopener">参考资料3</a></p>
<p>提问者从文档当中对数据是否加密产生了矛盾的结论。Intel的工程师回答”Data in EPC is encrypted and integrity protected “, 但之后又说”From the <strong>CPU standpoint</strong>, data in EPC is unencrypted, because the MEE sits transparently between the CPU and the PRM. In other words, the data in EPC is encrypted because it’s outside the CPU package. However, it doesn’t need to be this way. For instance, a CPU with special on-chip memory wouldn’t need the MEE and the EPC memory wouldn’t have to be encrypted.”</p>
</li>
</ul>
</li>
</ol>
<pre><code>对于这个问题我还需要再查一查。
</code></pre><ol start="5">
<li><p>对于EPC的R/W ops，OS是可以看见的？</p>
<p>（未找见相应资料。我推测如果操作是enclave与外界（memory）的话，肯定能被OS看见。在enclave内部的话，就看不见了）</p>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://chzhou.cc/2018/08/05/Spark Q&A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/05/Spark Q&A/" itemprop="url">Spark Q&A</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-05T11:29:21+08:00">
                2018-08-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li><p>Spark的shuffle类操作有哪些（除去groupbyKey)？这些操作是把partition 直接load到内存中吗？</p>
<blockquote>
<p>Operations which can cause a shuffle include <strong>repartition operations</strong> like repartition and coalesce, <strong>‘ByKey’ operations</strong> (except for counting) like groupByKey and reduceByKey, and <strong>join operations</strong> like cogroup and join <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#background" target="_blank" rel="noopener">RDD doc</a></p>
</blockquote>
<ul>
<li>repartition operations: <code>repartition</code>, <code>coalesce</code></li>
<li>‘ByKey’ operations: <code>groupByKey</code>, <code>reduceByKey</code>, <code>aggregateByKey</code>, <code>sortByKey</code>  (<code>countByKey</code> is an <em>Actions</em> operation, so it isn’t a <em>shuffle operation</em>)</li>
<li>join operations: <code>cogroup</code>, <code>join</code></li>
<li><p>Another operation which takes “numPartitions” as an argument is <code>distinct</code> operation</p>
<p>RDD is stored in memory by default. There are  seven storage level. The full set of storage levels can be found <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence" target="_blank" rel="noopener">here</a>. <em>MEMORY_ONLY</em> is the default level, which means when data can’t fit in memory, <strong>some partitions will not be cached</strong> and will be <strong>recomputed</strong> on the fly each time they’re needed. In <em>MEMORY_AND_DISK</em> level, if the RDD does not fit in memory, store the partitions that don’t fit on disk, and read them from there when they’re needed. Also, shuffle generates a large number of intermediate files on disk, these files are preserved until the corresponding RDDs are no longer used and are garbage collected. This is done so the shuffle files don’t need to be re-created if the lineage is re-computed.</p>
<p>（有个问题：刚开始的资料来自RDD doc，在这个shuffle类目<a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#performance-impact" target="_blank" rel="noopener">链接</a>的第三段最后一句，原文是说内存不够的话就会把tables存到disk中。这里说的只是针对’ByKey操作吗？（因为上文在说ByKey操作））</p>
</li>
</ul>
</li>
<li><p>Spark中所谓的lazy transformation触发条件有哪些？</p>
<blockquote>
<p>The transformations are only computed when an action requires a result to be returned to the driver program. <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations" target="_blank" rel="noopener">RDD doc</a></p>
</blockquote>
<p> <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions" target="_blank" rel="noopener">Actions操作</a></p>
</li>
<li><p>Spark core API</p>
<p> Spark Core提供Spark最基础与最核心的功能，主要包括以下功能：</p>
<ul>
<li>SparkContext：通常而言，Driver Application的执行与输出都是通过SparkContext来完成的。在正式提交Application之前，首先需要初始化SparkContext。SparkContext隐藏了网络通信、分布式部署、消息通信、存储能力、计算能力、缓存、测量系统、文件服务、Web服务等内容，应用程序开发者只需要使用SparkContext提供的API完成功能开发。SparkContext内置的DAGScheduler负责创建Job，将DAG中的RDD划分到不同的Stage，提交Stage等功能。内置的TaskScheduler负责资源的申请，任务的提交及请求集群对任务的调度等工作。 </li>
<li>存储体系：Spark优先考虑使用各节点的内存作为存储，当内存不足时才会考虑使用磁盘，这极大地减少了磁盘IO，提升了任务执行的效率，使得Spark适用于实时计算、流式计算等场景。此外，Spark还提供了以内存为中心的高容错的分布式文件系统Tachyon供用户进行选择。Tachyon能够为Spark提供可靠的内存级的文件共享服务。 </li>
<li>计算引擎：计算引擎由SparkContext中的DAGScheduler、RDD以及具体节点上的Executor负责执行的Map和Reduce任务组成。DAGScheduler和RDD虽然位于SparkContext内部，但是在任务正式提交与执行之前会将Job中的RDD组织成有向无环图（DAG），并对Stage进行划分，决定了任务执行阶段任务的数量、迭代计算、shuffle等过程。 </li>
<li>部署模式：由于单节点不足以提供足够的存储和计算能力，所以作为大数据处理的Spark在SparkContext的TaskScheduler组件中提供了对Standalone部署模式的实现和Yarn、Mesos等分布式资源管理系统的支持。通过使用Standalone、Yarn、Mesos等部署模式为Task分配计算资源，提高任务的并发执行效率。</li>
</ul>
</li>
<li><p>Where is RDD’s lineage stored? And how to get it?</p>
<blockquote>
<p>The RDD‘s lineage is stored in memory, same as RDD. And the RDD lineage lives on the driver where RDDs live. <a href="https://stackoverflow.com/questions/34713793/where-spark-rdd-lineage-is-stored" target="_blank" rel="noopener">stackoverflow</a></p>
</blockquote>
<p> To get lineage:</p>
<ol>
<li><p>Using <code>toDebugString</code> method, one can get RDD lineage graph.  (<a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-lineage.html#toDebugString" target="_blank" rel="noopener">参考</a>)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> wordCount = sc.textFile(<span class="string">"README.md"</span>).flatMap(_.split(<span class="string">"\\s+"</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">wordCount: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">21</span>] at reduceByKey at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; wordCount.toDebugString</span><br><span class="line">res13: <span class="type">String</span> =</span><br><span class="line">(<span class="number">2</span>) <span class="type">ShuffledRDD</span>[<span class="number">21</span>] at reduceByKey at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line"> +-(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">20</span>] at map at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line">    |  <span class="type">MapPartitionsRDD</span>[<span class="number">19</span>] at flatMap at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line">    |  <span class="type">README</span>.md <span class="type">MapPartitionsRDD</span>[<span class="number">18</span>] at textFile at &lt;console&gt;:<span class="number">24</span> []</span><br><span class="line">    |  <span class="type">README</span>.md <span class="type">HadoopRDD</span>[<span class="number">17</span>] at textFile at &lt;console&gt;:<span class="number">24</span> []</span><br></pre></td></tr></table></figure>
</li>
<li><p>In spark shell, with <em>spark.logLineage</em> property enabled , <code>toDebugString</code> is included when executing an action.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-shell --conf spark.logLineage=true</span><br></pre></td></tr></table></figure>
</li>
<li><p>Using <code>spark-submit</code> </p>
<p>This section is still in progress…( Because using <code>--conf spark.logLineage=true</code>, the console doesn`t print the graph.)</p>
<p>And this is the <code>runjob</code> method’s souce code in SparkContext class.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"SparkContext has been shutdown"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> callSite = getCallSite</span><br><span class="line">    <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">    logInfo(<span class="string">"Starting job: "</span> + callSite.shortForm)</span><br><span class="line">    <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.logLineage"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">      logInfo(<span class="string">"RDD's recursive dependencies:\n"</span> + rdd.toDebugString)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://chzhou.cc/2018/08/02/SparkML电影推荐流程分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/02/SparkML电影推荐流程分析/" itemprop="url">SparkML电影推荐流程分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-02T17:59:46+08:00">
                2018-08-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="SparkML电影推荐流程分析"><a href="#SparkML电影推荐流程分析" class="headerlink" title="SparkML电影推荐流程分析"></a>SparkML电影推荐流程分析</h1><p>之前采用<code>spark-submit</code> 进行分析，产出的信息太多，很难缕清关系，难以得到每步产生的数据和操作过程。所以采用<code>spark-shell</code> 以一行一行输入的方式交互进行程序运行，同时从Web-UI上产生的信息进行同步分析。以下为每步操作：</p>
<p>为了打字方便，以下 Web-UI统一拿 web 代替。</p>
<ol>
<li><p>启动<code>spark-shell</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chzhou@lotus02:/usr/spark/bin$ spark-shell --conf spark.logLineage=true --master spark://lotus02:7077 --deploy-mode client --jars file:///home/chzhou/mltr/machine-learning/target/scala-2.11/movielens-als_2.11-0.1.jar</span><br></pre></td></tr></table></figure>
<ul>
<li>操作的时候把程序用sbt打包的jar包导入，这样在shell里就可以调用原程序自定义的函数</li>
<li>指定master和deploy-mode，以分布式运行</li>
<li>使得<code>spark.logLineage</code> 为true，这样就能在控制台自动输出 RDD 的lineage</li>
</ul>
</li>
<li><p>导入库</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"><span class="keyword">import</span> scala.io.<span class="type">Source</span></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.<span class="type">Logger</span></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.<span class="type">Level</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.recommendation.&#123;<span class="type">ALS</span>, <span class="type">Rating</span>, <span class="type">MatrixFactorizationModel</span>&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>读取个人喜好的rating文件，并形成rdd</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> myRatings = <span class="type">MovieLensALS</span>.loadRatings(<span class="string">"/home/chzhou/ml-1m/personalRatings.txt"</span>)</span><br><span class="line">myRatings: <span class="type">Seq</span>[org.apache.spark.mllib.recommendation.<span class="type">Rating</span>] = <span class="type">Stream</span>(<span class="type">Rating</span>(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2.0</span>), ?)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> myRatingsRDD = sc.parallelize(myRatings, <span class="number">1</span>).cache</span><br><span class="line">myRatingsRDD: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.mllib.recommendation.<span class="type">Rating</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">39</span></span><br></pre></td></tr></table></figure>
<ul>
<li>第二句用<code>cache</code>将其存入内存，这样Web-UI中的Storage选项之后就可以查到RDD信息</li>
<li>此时Web-UI中还是全空的，没有任何信息，因为此时并没有Actions操作，并没有实际产生RDD</li>
</ul>
</li>
<li><p>在spark-shell中以多行输入的方式读取rating.dat文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"> <span class="keyword">val</span> ratings = sc.textFile(<span class="string">"hdfs://lotus02:9000/ml/medium/ratings.dat"</span>).map &#123; line =&gt;</span><br><span class="line">      <span class="keyword">val</span> fields = line.split(<span class="string">"::"</span>)</span><br><span class="line">      <span class="comment">// format: (timestamp % 10, Rating(userId, movieId, rating))</span></span><br><span class="line">      (fields(<span class="number">3</span>).toLong % <span class="number">10</span>, <span class="type">Rating</span>(fields(<span class="number">0</span>).toInt, fields(<span class="number">1</span>).toInt, fields(<span class="number">2</span>).toDouble))</span><br><span class="line">    &#125;.cache</span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">ratings: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Long</span>, org.apache.spark.mllib.recommendation.<span class="type">Rating</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">37</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在spark-shell中以多行输入的方式读取movies.dat文件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"><span class="keyword">val</span> movies = sc.textFile(<span class="string">"hdfs://lotus02:9000/ml/medium/movies.dat"</span>).map &#123; line =&gt;</span><br><span class="line">      <span class="keyword">val</span> fields = line.split(<span class="string">"::"</span>)</span><br><span class="line">      <span class="comment">// format: (movieId, movieName)</span></span><br><span class="line">      (fields(<span class="number">0</span>).toInt, fields(<span class="number">1</span>))</span><br><span class="line">    &#125;.cache</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">movies: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at map at &lt;console&gt;:<span class="number">37</span></span><br><span class="line"></span><br><span class="line">scala&gt; movies.collect().toMap</span><br><span class="line">res0: scala.collection.immutable.<span class="type">Map</span>[<span class="type">Int</span>,<span class="type">String</span>] = <span class="type">Map</span>(<span class="number">2163</span> -&gt; <span class="type">Attack</span> of the <span class="type">Killer</span> <span class="type">Tomatoes</span>! (<span class="number">1980</span>), <span class="number">645</span> -&gt; <span class="type">Nelly</span> &amp; <span class="type">Monsieur</span> <span class="type">Arnaud</span> (<span class="number">1995</span>), <span class="number">892</span> -&gt; <span class="type">Twelfth</span> <span class="type">Night</span> (<span class="number">1996</span>), <span class="number">69</span> -&gt; <span class="type">Friday</span> (<span class="number">1995</span>), <span class="number">2199</span> -&gt; <span class="type">Phoenix</span> (<span class="number">1998</span>), <span class="number">3021</span> -&gt; <span class="type">Funhouse</span>, <span class="type">The</span> (<span class="number">1981</span>), <span class="number">1322</span> -&gt; <span class="type">Amityville</span> <span class="number">1992</span>: <span class="type">It</span><span class="symbol">'s</span> <span class="type">About</span> <span class="type">Time</span> (<span class="number">1992</span>), <span class="number">1665</span> -&gt; <span class="type">Bean</span> (<span class="number">1997</span>), <span class="number">1036</span> -&gt; <span class="type">Die</span> <span class="type">Hard</span> (<span class="number">1988</span>), <span class="number">2822</span> -&gt; <span class="type">Medicine</span> <span class="type">Man</span> (<span class="number">1992</span>), <span class="number">2630</span> -&gt; <span class="type">Besieged</span> (<span class="type">L</span>' <span class="type">Assedio</span>) (<span class="number">1998</span>), <span class="number">3873</span> -&gt; <span class="type">Cat</span> <span class="type">Ballou</span> (<span class="number">1965</span>), <span class="number">1586</span> -&gt; <span class="type">G</span>.<span class="type">I</span>. <span class="type">Jane</span> (<span class="number">1997</span>), <span class="number">1501</span> -&gt; <span class="type">Keys</span> to <span class="type">Tulsa</span> (<span class="number">1997</span>), <span class="number">2452</span> -&gt; <span class="type">Gate</span> <span class="type">II</span>: <span class="type">Trespassers</span>, <span class="type">The</span> (<span class="number">1990</span>), <span class="number">809</span> -&gt; <span class="type">Fled</span> (<span class="number">1996</span>), <span class="number">1879</span> -&gt; <span class="type">Hanging</span> <span class="type">Garden</span>, <span class="type">The</span> (<span class="number">1997</span>), <span class="number">1337</span> -&gt; <span class="type">Body</span> <span class="type">Snatcher</span>, <span class="type">The</span> (<span class="number">1945</span>), <span class="number">1718</span> -&gt; <span class="type">Stranger</span> in the <span class="type">House</span> (<span class="number">1997</span>), <span class="number">2094</span> -&gt; <span class="type">Rocketeer</span>, <span class="type">The</span> (<span class="number">1991</span>), <span class="number">3944</span> -&gt; <span class="type">Bootmen</span> (<span class="number">2000</span>), <span class="number">1411</span> -&gt; <span class="type">Hamlet</span> (<span class="number">1996</span>), <span class="number">629</span> -&gt; <span class="type">Rude</span> (<span class="number">1995</span>), <span class="number">3883</span> -&gt; <span class="type">Catfish</span> in <span class="type">Black</span> <span class="type">Bean</span> <span class="type">Sauce</span> (<span class="number">2.</span>.</span><br></pre></td></tr></table></figure>
<ul>
<li><p>这里改写了原程序，原程序是直接进行了collect.toMap操作，这里分成两步，先cache存到内存中，再进行colletc.toMap操作</p>
</li>
<li><p>因为进行了collect操作，此时web显示了信息</p>
<p><img src="/images/SparkML电影推荐流程分析/s0.PNG" alt="s0"><br>为Stage0信息，进行了map操作。（绿色代表在内存中）</p>
</li>
<li><p>在web中stage/Aggregated Metrics by Executor选项中，可以看到</p>
<p><img src="/images/SparkML电影推荐流程分析/s0 reco.PNG" alt="s0 reco"></p>
<p>图片有些小。。。简单来说，就是movies.dat中总共有3883条record，这里08机器存了1951条record，09上存了3883-1951=1932条数据。在这里看出了数据的分布。从这个方面也显示了上图中map操作的时候从movies.dat[4]和movies.dat[5]中获得数据。（但是看不出来movies.dat[4]是08还是09上）</p>
</li>
<li><p>附上storage界面信息</p>
<p><img src="/images/SparkML电影推荐流程分析/s0 stor.PNG" alt="s0 stor"></p>
</li>
</ul>
</li>
<li><p>对ratings进行统计计数，先是对numRatings计数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> numRatings = ratings.count</span><br><span class="line">numRatings: <span class="type">Long</span> = <span class="number">1000209</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>输出ratings的数据总共有1000209条数据</p>
</li>
<li><p>从web上查看stage信息</p>
</li>
</ul>
</li>
</ol>
<pre><code>![s1](/images/SparkML电影推荐流程分析/s1.PNG)

显示cache的是MapPartitionsRDD[3]，这与在第4步中的控制台输出是一样的。
</code></pre><ul>
<li><p>查看slave存储</p>
<p><img src="/images/SparkML电影推荐流程分析/s1 rec.PNG" alt="s1 rec"></p>
<p>08上有503331条record，09上有1000209-503331=496878条数据</p>
</li>
<li><p>storage界面信息和上一步差不多，不截图显示了。</p>
</li>
</ul>
<ol start="7">
<li><p>接下来统计用户数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> numUsers = ratings.map(_._2.user).distinct.count</span><br><span class="line">numUsers: <span class="type">Long</span> = <span class="number">6040</span></span><br></pre></td></tr></table></figure>
<p>这里的map语句不太懂（推测是对ratings的字段进行操作）。。。然后进行了distinct操作，找出unique的用户，然后count进行计数。这里得到有6040名用户</p>
<p>这里为Job2，Job2里有两个stage，第一个是distinct操作，第二个是count操作。</p>
<ul>
<li><p>第一个是distinct操作</p>
<p><img src="/images/SparkML电影推荐流程分析/j2s1.PNG" alt="j2s1"></p>
<p>这里用了之前cache过的RDD[3]，然后进行map和distinct操作。</p>
<p>对于存储，不截图了，都一样，其中08上有3092条record，09上有2949条record（3092+2949=6041条，和上面输出的6040不一样。。？？）</p>
</li>
<li><p>第二个是count操作</p>
<p><img src="/images/SparkML电影推荐流程分析/j2说.PNG" alt="j2说"></p>
<p>（不懂为什么右上角是distinct，难道是在distinct里进行count操作，所以这么显示？？）</p>
<p> (在存储方面，08上是3020条record，09上是3021条record（加起来还是6041，为什么不是6040？？）</p>
</li>
</ul>
</li>
<li><p>接下来进行numMovies统计</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> numMovies = ratings.map(_._2.product).distinct.count</span><br><span class="line">numMovies: <span class="type">Long</span> = <span class="number">3706</span></span><br></pre></td></tr></table></figure>
<p>统计出来numMovies是3706部。</p>
<p>此时为Job3，和上一步一样，分为两个阶段，也是distinct和count操作。DAG图和上一步差不多。不截图了。在存储方面，distinct操作中08是3619条record，09上是3600条record。在count操作中08是3620条record，09是3599条数据。两个操作中的总数都是7219条record。</p>
</li>
<li><p>定义numPartions</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> numPartitions = <span class="number">4</span></span><br><span class="line">numPartitions: <span class="type">Int</span> = <span class="number">4</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>之后几步都是从ratings对数据进行切分，产生ML中的数据集。第一个数据集是训练集</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"><span class="keyword">val</span> training = ratings.filter(x =&gt; x._1 &lt; <span class="number">6</span>)</span><br><span class="line">      .values</span><br><span class="line">      .union(myRatingsRDD)</span><br><span class="line">      .repartition(numPartitions)</span><br><span class="line">      .cache()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">training: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.mllib.recommendation.<span class="type">Rating</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">21</span>] at repartition at &lt;console&gt;:<span class="number">45</span></span><br></pre></td></tr></table></figure>
<p>此时web中并没有变化，因为没有Actions操作。但是用cache将其存在了内存中。并且注意到和myRatingsRDD进行了union操作，并进行了repartition。</p>
</li>
<li><p>产生验证集</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> validation = ratings.filter(x =&gt; x._1 &gt;= <span class="number">6</span> &amp;&amp; x._1 &lt; <span class="number">8</span>)</span><br><span class="line">      .values</span><br><span class="line">      .repartition(numPartitions)</span><br><span class="line">      .cache()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">validation: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.mllib.recommendation.<span class="type">Rating</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">28</span>] at repartition at &lt;console&gt;:<span class="number">42</span></span><br></pre></td></tr></table></figure>
<p>web中还没有变化。进行了repartition。</p>
</li>
<li><p>产生测试集</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> test = ratings.filter(x =&gt; x._1 &gt;= <span class="number">8</span>).values.cache()</span><br><span class="line">test: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.mllib.recommendation.<span class="type">Rating</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">30</span>] at values at &lt;console&gt;:<span class="number">38</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>统计训练集大小</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> numTraining = training.count()</span><br><span class="line">numTraining: <span class="type">Long</span> = <span class="number">602252</span></span><br></pre></td></tr></table></figure>
<p>因为进行了count操作，此时web有repartition了信息。</p>
<p>为Job4，分为两个阶段，第一个为repartition，第二个是count操作。</p>
<ul>
<li><p>第一个为repartition，DAG图为</p>
<p><img src="/images/SparkML电影推荐流程分析/j4s1.PNG" alt="j4s1"></p>
<p>这里因为是对ratings操作，ratingsRDD已经cache过，所以直接读取，进行filter操作，然后与myRatings进行union，然后进行repartition。</p>
<p>存储方面，08上有303152条record，09上有299100条record，一共303152+299100=602252条record。</p>
</li>
<li><p>第二个是count操作，DAG图为</p>
<p><img src="/images/SparkML电影推荐流程分析/j4s2.PNG" alt="j4s2"></p>
<p>存储方面，08上有301126条record，09上有301126条record，一共301126+301126=602252条。</p>
</li>
<li><p>在web上的storage界面，显示Partitions已经为4：</p>
<p><img src="/images/SparkML电影推荐流程分析/j4stor.PNG" alt="j4stor"></p>
<p>前几个的RDD除了第一个rdd是一个partition，其他都是两个partition。RDD doc中关于partition是这样说的：“Normally, Spark tries to set the number of partitions automatically based on your cluster”。前几个都是spark自动生成的partition。</p>
</li>
</ul>
</li>
<li><p>统计验证集大小</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> numValidation = validation.count()</span><br><span class="line">numValidation: <span class="type">Long</span> = <span class="number">198919</span></span><br></pre></td></tr></table></figure>
<p>此时为Job5，与上一步一样，同样分为repartition和count操作。</p>
<ul>
<li><p>repartition操作DAG图为</p>
<p><img src="/images/SparkML电影推荐流程分析/j5s1.PNG" alt="j5s1"></p>
<p>因为没有上一步的union操作，所以这里直接从以前cache过的RDD[3]进行filter，repartition操作。存储方面，08上有100299条数据，09上有98620条数据，一共100299+98620=198919条数据。</p>
</li>
<li><p>count操作</p>
<p><img src="/images/SparkML电影推荐流程分析/j5s2.PNG" alt="j5s2"></p>
<p>数据方面，08上有99459条数据，09上有99460条数据，一共99459+99460=198919条数据。</p>
</li>
<li><p>在web上的storage界面，partitions同样为4。（未截图）</p>
</li>
</ul>
</li>
<li><p>统计测试集数据大小。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> numTest = test.count()</span><br><span class="line">numTest: <span class="type">Long</span> = <span class="number">199049</span></span><br></pre></td></tr></table></figure>
<p>此时为Job6阶段，因为对test没有进行repartition操作，这里只有count操作。</p>
<ul>
<li><p>DAG图为</p>
<p><img src="/images/SparkML电影推荐流程分析/j6s.PNG" alt="j6s"></p>
<p>在存储方面，08上有503331条record，09上有496878条record，一共503331+496878=1000209数据。（为什么与输出不一致？）</p>
<p>另外，在web的storage界面，因为没有repartition操作，产生的rdd[30]为两个partition。</p>
</li>
</ul>
</li>
<li><p>接下来是准备训练的一些参数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> ranks = <span class="type">List</span>(<span class="number">8</span>, <span class="number">12</span>)</span><br><span class="line">ranks: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">8</span>, <span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> lambdas = <span class="type">List</span>(<span class="number">0.1</span>, <span class="number">10.0</span>)</span><br><span class="line">lambdas: <span class="type">List</span>[<span class="type">Double</span>] = <span class="type">List</span>(<span class="number">0.1</span>, <span class="number">10.0</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> numIters = <span class="type">List</span>(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">numIters: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> bestModel: <span class="type">Option</span>[<span class="type">MatrixFactorizationModel</span>] = <span class="type">None</span></span><br><span class="line">bestModel: <span class="type">Option</span>[org.apache.spark.mllib.recommendation.<span class="type">MatrixFactorizationModel</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> bestValidationRmse = <span class="type">Double</span>.<span class="type">MaxValue</span></span><br><span class="line">bestValidationRmse: <span class="type">Double</span> = <span class="number">1.7976931348623157E308</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> bestRank = <span class="number">0</span></span><br><span class="line">bestRank: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> bestLambda = <span class="number">-1.0</span></span><br><span class="line">bestLambda: <span class="type">Double</span> = <span class="number">-1.0</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> bestNumIter = <span class="number">-1</span></span><br><span class="line">bestNumIter: <span class="type">Int</span> = <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<p>此时没有rdd产生，web上没有变化。</p>
</li>
<li><p>此时进行了模型训练，调用ML库中的ALS（交替最小二乘 alternating least squares）。此时产生了很多的操作，且数据不清晰，有大量的矩阵操作） </p>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://chzhou.cc/2018/06/04/creation-code-input-data-solc编译出来的code，这三种code有什么区别/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/04/creation-code-input-data-solc编译出来的code，这三种code有什么区别/" itemprop="url">creation code, input data, solc编译出来的code，这三种code有什么区别</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-04T23:27:11+08:00">
                2018-06-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="三种code"><a href="#三种code" class="headerlink" title="三种code"></a>三种code</h2><ul>
<li><p>creation code</p>
<ul>
<li>在后添加了Constructor Arguments </li>
</ul>
</li>
<li><p>input data</p>
<ul>
<li><p>在用命令创建contract的时候需要输入的data</p>
</li>
<li><p>格式为bin</p>
</li>
<li><p><a href="https://medium.com/@gus_tavo_guim/deploying-a-smart-contract-the-hard-way-8aae778d4f2a" target="_blank" rel="noopener">根据这个blog</a>和<a href="https://github.com/ethereum/go-ethereum/wiki/Contract-Tutorial" target="_blank" rel="noopener">Geth文档</a>，Solc编译出来的bin code作为参数传入创建合约的命令中。</p>
<ul>
<li><p>Medium blog</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> deployTransationObject = &#123; <span class="attr">from</span>: eth.accounts[<span class="number">0</span>], <span class="attr">data</span>: storageBinCode, <span class="attr">gas</span>: <span class="number">1000000</span> &#125;;</span><br><span class="line"><span class="keyword">var</span> storageInstance = storageContract.new(deployTransationObject)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Geth 文档</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> greeter = greeterContract.new(_greeting,&#123;<span class="attr">from</span>:web3.eth.accounts[<span class="number">0</span>], <span class="attr">data</span>: greeterCompiled.greeter.code, <span class="attr">gas</span>: <span class="number">1000000</span>&#125;, <span class="function"><span class="keyword">function</span>(<span class="params">e, contract</span>)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>solc</p>
<ul>
<li>solc的关于编译的有两个arguments，其中一个是 <code>--bin</code>，解释为Binary of the contracts in hex，另外一个是<code>--bin-runtime</code>，解释为Binary of the runtime part of the contracts in hex。</li>
<li>根据这个<a href="https://ethereum.stackexchange.com/questions/13086/solc-bin-vs-bin-runtime" target="_blank" rel="noopener">回答</a>，<code>--bin-runtime</code> is the code that is actually placed on the blockchain. The regular <code>--bin</code> output is the code placed on the blockchain <strong>plus</strong> the code needed to get this code placed on the blockchain</li>
<li>上面的Medium blog是调用了<code>--bin</code> 命令，而Geth官方文档用的是web3.eth.compile.solidity命令，推测也应该用的是<code>--bin</code>。（未证实）</li>
</ul>
</li>
</ul>
<h2 id="后续进展"><a href="#后续进展" class="headerlink" title="后续进展"></a>后续进展</h2><ul>
<li><p>creation code 与 input data</p>
<p>在Etherscan上寻找几个verified的合约进行验证，得出<strong>creation code</strong>和<strong>input data</strong>是完全一致的。代码由<strong>三部分</strong>组成，第一部分是前面的一些数字，代表着初始化合约的init过程。第二部分是合约的主体过程。第三部分是合约的Constructor Arguments，被添加到了最后。</p>
</li>
<li><p>solc</p>
<p>在stack exchange上进行了<a href="https://ethereum.stackexchange.com/questions/50180/whats-the-contract-creation-code-in-etherscan-verfied-contract" target="_blank" rel="noopener">提问</a>，问题是creation code和solc编译出来的code有何区别。回答是Contract Creation Code is the full bytecode from what contract was deployed, <strong>including constructor parameters</strong>。如果合约没有constructor parameters的话，那么这两种code都是一致的。</p>
</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><p><a href="https://etherscancom.freshdesk.com/support/solutions/articles/35000022165-contract-verification-constructor-arguments" target="_blank" rel="noopener">https://etherscancom.freshdesk.com/support/solutions/articles/35000022165-contract-verification-constructor-arguments</a></p>
</li>
<li><p><a href="https://ethereum.stackexchange.com/questions/13086/solc-bin-vs-bin-runtime" target="_blank" rel="noopener">https://ethereum.stackexchange.com/questions/13086/solc-bin-vs-bin-runtime</a></p>
</li>
<li><p><a href="https://medium.com/@gus_tavo_guim/deploying-a-smart-contract-the-hard-way-8aae778d4f2a" target="_blank" rel="noopener">https://medium.com/@gus_tavo_guim/deploying-a-smart-contract-the-hard-way-8aae778d4f2a</a></p>
</li>
<li><p><a href="https://github.com/ethereum/go-ethereum/wiki/Contract-Tutorial" target="_blank" rel="noopener">https://github.com/ethereum/go-ethereum/wiki/Contract-Tutorial</a></p>
</li>
<li><p><a href="https://ethereum.stackexchange.com/questions/50180/whats-the-contract-creation-code-in-etherscan-verfied-contract" target="_blank" rel="noopener">https://ethereum.stackexchange.com/questions/50180/whats-the-contract-creation-code-in-etherscan-verfied-contract</a></p>
</li>
<li><p>验证creation code和input data是否一致的几个合约</p>
<ul>
<li><a href="https://etherscan.io/address/0xcac337492149bdb66b088bf5914bedfbf78ccc18#code" target="_blank" rel="noopener">https://etherscan.io/address/0xcac337492149bdb66b088bf5914bedfbf78ccc18#code</a></li>
<li><a href="https://etherscan.io/address/0x7c333b69021b3ad9288d3b0083f9bd27c6d4680a#code" target="_blank" rel="noopener">https://etherscan.io/address/0x7c333b69021b3ad9288d3b0083f9bd27c6d4680a#code</a></li>
<li><a href="https://etherscan.io/address/0x233d2daad4018fae14c69b2830bf97057c7fb1b5#code" target="_blank" rel="noopener">https://etherscan.io/address/0x233d2daad4018fae14c69b2830bf97057c7fb1b5#code</a></li>
</ul>
<p>注：这三个合约的最后都有Constructor Arguments ，不知这个有没有影响导致两种code一致。但是现在比较新的合约在verify的时候都需要提供Constructor Arguments，所以就不加考虑这个因素以及旧的没有Constructor Arguments的合约。</p>
<p>注：在最新的verified contract里面，检查了最新的前三个合约的两种code，是完全一致的。</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://chzhou.cc/2018/05/31/DeepLab-v3-predition-out-of-bound/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zhou">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/31/DeepLab-v3-predition-out-of-bound/" itemprop="url">DeepLab v3+: prediction out of bound</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-31T23:57:48+08:00">
                2018-05-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="出现情景"><a href="#出现情景" class="headerlink" title="出现情景"></a>出现情景</h2><p>最近在用DeepLab v3+ 训练模型，已经训练好了自己的数据集。可是在验证的时候，程序总是报错。抛出<code>prediction out of bound</code> 的错误。意思很好理解，就是预测超出了范围。但是范围是什么呢？又是如何超出的呢？经过搜索，找出了答案。</p>
<h2 id="文件代码"><a href="#文件代码" class="headerlink" title="文件代码"></a>文件代码</h2><p>在/deeplab/datasets文件夹下，有一个名为 <em>segmentation_dataset.py</em> 的文件。在该文件夹里，就定义了训练集和验证集的信息。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">_PASCAL_VOC_SEG_INFORMATION = DatasetDescriptor(</span><br><span class="line">    splits_to_sizes=&#123;</span><br><span class="line">        <span class="string">'train'</span>: <span class="number">7</span>,</span><br><span class="line">        <span class="string">'trainval'</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">'val'</span>: <span class="number">3</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">    num_classes=<span class="number">8</span>,</span><br><span class="line">    ignore_label=<span class="number">255</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这里已经针对自己的数据集进行了修改。其中<code>train</code> 和 <code>val</code>的字段意思就是对应数据集的大小。因为我只是测试，所以这里我的训练集就只有7张，验证集是3张。下面的<code>num_classes</code>和<code>ignore_lable</code>是数据集的类别数目和忽视的类别。而我出问题的就在这个<code>num_classes</code>上。</p>
<h2 id="问题来源"><a href="#问题来源" class="headerlink" title="问题来源"></a>问题来源</h2><p>直觉认为这里<code>num_classes</code>就是类别的数目，当然这个想法也是对的。但是这里的前提是<strong>数据集的lable标记是从1开始的</strong>，也就是说，你的类别从1，2，3，…，num_classes这样定义的。但是这个是很反人类的，因为有的时候为了更加直观理解，并不一定从1开始。比如这次百度提供的数据集，车的标记就是33。而我的写的<code>num_classes</code> 是8，自然33要大于8，就抛出了<code>prediction out of bound</code>的错误了。</p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>在查到这个问题后，将自己的<code>num_classes</code>变成了这次数据集对应的lableID，但是这次百度给的数据集的id是乘以1000的，所以车的id33，就变成了33000。我这样改之后，训练的时候又爆出了<code>OOM</code> 的问题，也就是说训练爆内存了，显然是因为<code>num_classes</code>的数目太大。</p>
<p>这样就涉及到了修改对应的id问题，也就是说把车的id经过修改变成1。<a href="https://gist.github.com/DrSleep/4bce37254c5900545e6b65f6a0858b9c" target="_blank" rel="noopener">具体的方案在这里</a>。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://github.com/tensorflow/models/issues/3906中@BillBai的回答" target="_blank" rel="noopener">https://github.com/tensorflow/models/issues/3906中@BillBai的回答</a></li>
<li><a href="https://gist.github.com/DrSleep/4bce37254c5900545e6b65f6a0858b9c" target="_blank" rel="noopener">https://gist.github.com/DrSleep/4bce37254c5900545e6b65f6a0858b9c</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Zhou">
          <p class="site-author-name" itemprop="name">Zhou</p>
           
              <p class="site-description motion-element" itemprop="description">周的小站</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/archive/index.html">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou</span>

  
</div>


  <div class="powered-by">
    由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
  </div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">
    主题 &mdash;
    <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
      NexT.Mist
    </a>
  </div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  








  





  

  

  

  

  

  

</body>
</html>
